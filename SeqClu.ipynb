{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accelerating cluster assignment for SeqClu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SeqClu is a real-time sequence clustering using online K-medoids algorithm. This notebook introduces a new algorithm to improve the cluster assignment with 3 variants.\n",
    "\n",
    "Make sure to install the requirements from requirements.txt before executing this notebook.\n",
    "\n",
    "Jump to the [Execution](#execution) section to set the parameters and execute the notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents:\n",
    "* [Helper classes](#helper_classes)\n",
    "* [Loading Datasets](#loading_datasets)\n",
    "    * [Toy Dataset](#toy_dataset)\n",
    "    * [Handwirtten Dataset](#handwritten_dataset)\n",
    "    * [Synthetic Control Dataset](#sc_dataset)\n",
    "* [Shared Implementation](#shared_implementation)\n",
    "* [SeqClu base implementation](#base)\n",
    "* [SeqClu improved cluster assignment algorithm](#new)\n",
    "    * [Variant 1](#variant1)\n",
    "    * [Variant 2](#variant2)\n",
    "    * [Variant 3](#variant3)\n",
    "* [The experiment](#experiment)\n",
    "    * [Hypothesis testing](#hypothesis_testing)\n",
    "    * [Plotting results](#plotting)\n",
    "* [Execution](#execution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import math\n",
    "import operator\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from io import TextIOWrapper\n",
    "from statistics import mean, stdev\n",
    "from typing import Any, Dict, List, Tuple\n",
    "from zipfile import ZipFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastdtw import fastdtw\n",
    "from scipy.spatial.distance import euclidean\n",
    "from scipy.stats import wilcoxon\n",
    "from sklearn.metrics import f1_score\n",
    "from tqdm.auto import tqdm\n",
    "from tslearn.clustering import silhouette_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper classes <a class=\"anchor\" id=\"helper_classes\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those helper classes are needed for running SeqClu and carrying out the experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    \"\"\"Dataset class used to encapsulate a dataset and its labels.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, name: str, points: List, ids: List, labels: List, nclasses: int = None) -> None:\n",
    "        \"\"\"The constructor for the Dataset class.\n",
    "        Args:\n",
    "            name (str): The given name of the dataset. Used to distinguish which dataset is loaded in the Dataset object.\n",
    "            points (List): The list of sequences.\n",
    "            ids (List): The list of ids. [0....n].\n",
    "            labels (List): The list of labels.\n",
    "            nclasses (int, optional): The number of classes in the dataset. Gets calculated if it is None. Defaults to None.\n",
    "        \"\"\"\n",
    "        self.name = name\n",
    "        self.points = points\n",
    "        self.ids = ids\n",
    "        self.labels = labels\n",
    "        self.nclasses = nclasses if nclasses is not None else len(set(self.labels))\n",
    "\n",
    "    def shuffle(self, nclasses: int, nprototypes: int) -> None:\n",
    "        \"\"\"The shuffle method is used to shuffle the data while maintaining the first n * p sequences.\n",
    "        Args:\n",
    "            nclasses (int): The number of classes used (n).\n",
    "            nprototypes (int): The number of prototypes used (p).\n",
    "        \"\"\"\n",
    "        f_points = self.points[0: nclasses * nprototypes]\n",
    "        s_points = self.points[nclasses * nprototypes:]\n",
    "        f_ids = self.ids[0: nclasses * nprototypes]\n",
    "        s_ids = self.ids[nclasses * nprototypes:]\n",
    "        f_labels = self.labels[0: nclasses * nprototypes]\n",
    "        s_labels = self.labels[nclasses * nprototypes:]\n",
    "        s = list(zip(s_points, s_ids, s_labels))\n",
    "        random.shuffle(s)\n",
    "        s_points, s_ids, s_labels = zip(*s)\n",
    "        self.points = f_points + list(s_points)\n",
    "        self.ids = f_ids + list(s_ids)\n",
    "        self.labels = f_labels + list(s_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Measurements:\n",
    "    \"\"\"Measurements class used to store the samples of a metric.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, metric: str, samples: List) -> None:\n",
    "        \"\"\"The constructor for the Measurements class.\n",
    "        Args:\n",
    "            metric (str): The name of the metric.\n",
    "            samples (List): The list of samples.\n",
    "        \"\"\"        \n",
    "        self.metric = metric\n",
    "        self.samples = samples\n",
    "        # Calculate the mean and stdev\n",
    "        self.mean = mean(samples)\n",
    "        self.std = stdev(samples)\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        \"\"\"The toString method for Measurements class.\n",
    "        Returns:\n",
    "            str: The String representation of the measurements class.\n",
    "        \"\"\"        \n",
    "        return self.metric + \"{mean=\" + str(self.mean) + \", std=\" + str(self.std) + \"}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Variant:\n",
    "    \"\"\"The Variant class used to represent an implementation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, variant: str, phi: float = -1.0) -> None:\n",
    "        \"\"\"The constructor for the Variant class.\n",
    "        Args:\n",
    "            variant (str): The name of the Variant (Original, Variant 1, Variant 2, Variant 3).\n",
    "            phi (float, optional): The phi value for the variant. Leave empty if variant is Original. Defaults to -1.0.\n",
    "        \"\"\"        \n",
    "        self.variant = variant\n",
    "        self.phi = phi\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        \"\"\"The __str__ methode of the Variant class.\n",
    "        Returns:\n",
    "            str: The String representation of the variant class.\n",
    "        \"\"\"        \n",
    "        if self.phi == -1.0:\n",
    "            return self.variant\n",
    "        return self.variant + \" with \\u03A6 = \" + str(self.phi)\n",
    "\n",
    "    def __eq__(self, other: Any) -> bool:\n",
    "        \"\"\"Equals method for the variant class.\n",
    "        Args:\n",
    "            other (Any): The other variant.\n",
    "        Returns:\n",
    "            bool: Whethers two variants objects are the same.\n",
    "        \"\"\"        \n",
    "        if isinstance(other, Variant):\n",
    "            if other.variant == self.variant and other.phi == self.phi:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def __hash__(self):\n",
    "        \"\"\"The hash methode for the Variant class.\n",
    "        Returns:\n",
    "            [type]: The resultant hash.\n",
    "        \"\"\"        \n",
    "        return hash((self.variant, self.phi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stats:\n",
    "    \"\"\"The Stats class used to store measurements for different metrics.\n",
    "    \"\"\"    \n",
    "    def __init__(self, name: str, measurements: Dict[str, Measurements]) -> None:\n",
    "        \"\"\"The constructor for the Stats class.\n",
    "        Args:\n",
    "            name (str): The name of the variant (Original, Variant 1, Variant 2, Variant 3).\n",
    "            measurements (Dict[str, Measurements]): The metrics with their measurements.\n",
    "        \"\"\"        \n",
    "        self.name = name\n",
    "        self.measurements = measurements\n",
    "    def __str__(self) -> str:\n",
    "        \"\"\"The __str__ methode of the Stats class.\n",
    "        Returns:\n",
    "            str: The String representation of the Stats class.\n",
    "        \"\"\"        \n",
    "        line = self.name + \" {\\n\\t\"\n",
    "        pairs = list(self.measurements.items())\n",
    "        for i in range(len(pairs)):\n",
    "            line += str(pairs[i][1]) + \",\\n\\t\" if i != len(pairs) - 1 else str(pairs[i][1]) + \"\\n}\"\n",
    "        return line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Datasets <a class=\"anchor\" id=\"loading_datasets\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are multiple datasets that can used to experiment on SeqClu which can be found below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Toy data-set <a class=\"anchor\" id=\"toy_dataset\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateCurve(n: float, _freq: Tuple[float, float], samplingrate: int, err: float, phase: int) -> List[np.ndarray]:\n",
    "    \"\"\"Function used to create the curve for the sine dataset.\n",
    "    Args:\n",
    "        n (float): The number of points.\n",
    "        _freq (Tuple[float, float]): The frequency.\n",
    "        samplingrate (int): The sampling rate.\n",
    "        err (float): The error.\n",
    "        phase (int): The phase.\n",
    "    Returns:\n",
    "        List[np.ndarray]: The generated points.\n",
    "    \"\"\"\n",
    "    trajectory = []\n",
    "    n = int(n)\n",
    "    for i in range(n):\n",
    "        freq = random.uniform(_freq[0], _freq[1])\n",
    "        line = np.arange(1, 101, samplingrate)\n",
    "        error = [random.random() * err for x in range(len(line))]\n",
    "        l = np.sin((freq * line) + phase) + error\n",
    "        trajectory.append(l)\n",
    "    return trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateToyDataset(nclasses: int, nprototypes: int, n_samples: int = 50, samplingrate: int = 1) -> Dataset:\n",
    "    \"\"\"Function used to generate the toy dataset.\n",
    "    Args:\n",
    "        nclasses (int): The number of classes.\n",
    "        nprototypes (int): The number of prototypes.\n",
    "        n_samples (int, optional): The number of points. Defaults to 50.\n",
    "        samplingrate (int, optional): The sampling rate. Defaults to 1.\n",
    "    Returns:\n",
    "        Dataset: The Dataset object that contains the data and the labels.\n",
    "    \"\"\"\n",
    "\n",
    "    c1 = generateCurve(n_samples / nclasses, (0.1, 0.12), samplingrate, 0.2, 5)\n",
    "    c2 = generateCurve(n_samples / nclasses, (0.2, 0.22), samplingrate, 0.4, 12)\n",
    "    c3 = generateCurve(n_samples / nclasses, (0.2, 0.22), samplingrate, 0.4, -10)\n",
    "    classes = list(range(nclasses))\n",
    "    trajectory = []\n",
    "    trajectory.extend(list(zip(c1[0:nprototypes], [classes[0]] * len(c1))))\n",
    "    trajectory.extend(list(zip(c2[0:nprototypes], [classes[1]] * len(c2))))\n",
    "    trajectory.extend(list(zip(c3[0:nprototypes], [classes[2]] * len(c3))))\n",
    "    randlist = []\n",
    "    randlist.extend(list(zip(c1[nprototypes:], [classes[0]] * len(c1))))\n",
    "    randlist.extend(list(zip(c2[nprototypes:], [classes[1]] * len(c2))))\n",
    "    randlist.extend(list(zip(c3[nprototypes:], [classes[2]] * len(c3))))\n",
    "    random.shuffle(randlist)\n",
    "    trajectory.extend(randlist)\n",
    "    X1 = [x for (x, y) in trajectory]  # Data\n",
    "    ann1 = [x for x, y in enumerate(X1)]  # IDs\n",
    "    labs1 = [y for (x, y) in trajectory]  # classes\n",
    "    return Dataset(\"toy dataset\", X1, ann1, labs1, nclasses = nclasses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UCI Hand-written character dataset <a class=\"anchor\" id=\"handwritten_dataset\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The UCI hand-written character dataset contains the data of handwritten characters. You can choose which characters you want to use. More info about the dataset can be found [here](https://archive.ics.uci.edu/ml/datasets/optical+recognition+of+handwritten+digits)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseFile(lines: List[str]) -> Dict[str, str]:\n",
    "    \"\"\"Function used to parse the file for the handwritten dataset.\n",
    "    Args:\n",
    "        lines (List[str]): The lines read from the file.\n",
    "    Returns:\n",
    "        Dict[str, str]: The characters and their raw data.\n",
    "    \"\"\"\n",
    "    points: Dict[str, str] = dict()\n",
    "    newchar = False\n",
    "    cont = False\n",
    "    point = []\n",
    "    cclass = None\n",
    "    for line in lines[1:]:\n",
    "        if '.COMMENT' in line and 'Class' in line and '[' in line and '#' not in line:\n",
    "            b = re.findall('.*?\\.COMMENT\\s+Class\\s+\\[(.*?)\\]', line)\n",
    "            cclass = b[0]\n",
    "            newchar = True\n",
    "            point = []\n",
    "            continue\n",
    "        if '.PEN_UP' in line:\n",
    "            cont = False\n",
    "            if cclass not in points.keys():\n",
    "                points[cclass] = []\n",
    "            points[cclass].append(point)\n",
    "        if '.PEN_DOWN' in line:\n",
    "            cont = True\n",
    "            continue\n",
    "        if newchar and cont:\n",
    "            b = re.findall('.*?(\\d+)\\s+([-\\d]+).*', line)\n",
    "            xy = b[0]\n",
    "            point.append((int(xy[0]), int(xy[1])))\n",
    "    return points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateHandWrittenDataset(nprototypes: int, classes: List[chr] = ['O', '2', '9']) -> Dataset:\n",
    "    \"\"\"Generates the handwritten dataset.\n",
    "    Args:\n",
    "        nprototypes (int): The number of prototypes.\n",
    "        classes (List[chr], optional): The characters to be used. Defaults to ['O', '2', '9'].\n",
    "    Returns:\n",
    "        Dataset: The Dataset object that contains the data and the labels.\n",
    "    \"\"\"\n",
    "    # classes = ['C', 'U', 'V', 'W', 'S', 'O', '1', '2', '3', '5', '6', '8', '9']\n",
    "    segments = dict()\n",
    "    archive = ZipFile('handwriting.zip', 'r')\n",
    "    files = archive.namelist()\n",
    "    for f in files:\n",
    "        f_ = TextIOWrapper(archive.open(f), encoding=\"utf-8\")\n",
    "        lines = f_.readlines()\n",
    "        content = parseFile(lines)\n",
    "        for cclass, segment in content.items():\n",
    "            if cclass not in classes:\n",
    "                continue\n",
    "            if cclass not in segments.keys():\n",
    "                segments[cclass] = []\n",
    "            segments[cclass].extend(segment)\n",
    "    archive.close()\n",
    "    selected_segments = []\n",
    "    for c in classes:\n",
    "        selected_segments.append(segments[c])\n",
    "\n",
    "    # Preparing input sequence data (Exp with different settings, e.g. randomize, inverted, etc)\n",
    "    trajectory = []\n",
    "    randlist = []\n",
    "    for i in range(len(classes)):\n",
    "        trajectory.extend(list(zip(selected_segments[i][0:nprototypes], [classes[i]] * len(selected_segments[i]))))\n",
    "        # randomize incoming sequences so they belong to random classes\n",
    "        randlist.extend(list(zip(selected_segments[i][nprototypes:], [classes[i]] * len(selected_segments[i]))))\n",
    "    random.shuffle(randlist)\n",
    "    trajectory.extend(randlist)\n",
    "    # First 15 points are the prototypes\n",
    "    X2 = [x for (x, y) in trajectory]  # Data\n",
    "    ann2 = [x for x, y in enumerate(X2)]  # IDs\n",
    "    labs2 = [y for (x, y) in trajectory]  # classes\n",
    "    adjusted_labels = []\n",
    "    conversion = {}\n",
    "    counter = 0\n",
    "    for l in labs2:\n",
    "        if l in conversion:\n",
    "            adjusted_labels.append(conversion[l])\n",
    "        else:\n",
    "            conversion[l] = counter\n",
    "            adjusted_labels.append(counter)\n",
    "            counter += 1\n",
    "    return Dataset(\"handwritten dataset\", X2, ann2, adjusted_labels, nclasses = len(classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synthetic Control Data-set <a class=\"anchor\" id=\"fish_dataset\"></a>"
   ]
  },
  {
   "source": [
    "The synthetic control dataset contains 6 classes each with 50 objects. The sequences have a length of 60. The dataset has been sorted and stored as npy file for efficient retrieval and initialization. The dataset was obtained from [here](https://www.cs.ucr.edu/~eamonn/time_series_data_2018/)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generateSynthicControlDS(nprototypes: int) -> Dataset:\n",
    "    \"\"\"Function used to generate the syntheticControl dataset.\n",
    "    Args:\n",
    "        nprototypes (int): The number of prototypes\n",
    "    Returns:\n",
    "        Dataset: The Dataset object that contains the data and the labels.\n",
    "    \"\"\"\n",
    "    data = np.load(\"sc_data.npy\").tolist()\n",
    "    assert len(data) == 300\n",
    "    # The dataset is sorted by label.\n",
    "    pairs = {0: data[0:50], 1: data[50:100], 2: data[100:150], 3: data[150:200], 4: data[200:250], 5: data[250:300]}\n",
    "    # Split the dataset so the first part is used for initialization and second part is shuffled and is used for clustering.\n",
    "    f_data = []\n",
    "    f_labels = [i for i in range(6) for _ in range(nprototypes)]\n",
    "    s_data = []\n",
    "    s_labels = [i for i in range(6) for _ in range(50- nprototypes)]\n",
    "    for i in range(6):\n",
    "        f_data += pairs[i][0:nprototypes]\n",
    "        s_data += pairs[i][nprototypes:]\n",
    "    s = list(zip(s_data, s_labels))\n",
    "    random.shuffle(s)\n",
    "    s_data, s_labels = zip(*s)\n",
    "    ann = [x for x, y in enumerate(data)]\n",
    "    output = Dataset(\"synthetic control dataset\", f_data + list(s_data), ann, f_labels + list(s_labels), nclasses = 6)\n",
    "    assert len(output.labels) == 300 and len(output.points) == 300 \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shared Implementation <a class=\"anchor\" id=\"shared_implementation\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section contains the shared implementation that is used by the different versions of the algorithm. The SeqClu class has all the functionality needed to implement every version of SeqClu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeqClu(ABC):\n",
    "    \"\"\"The shared (abstract) SeqClu class that's used for all implementations.\n",
    "    Args:\n",
    "        ABC ([type]): Declared as an abstract class.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset: Dataset, nclasses: int, nprototypes: int) -> None:\n",
    "        \"\"\"The Constructor for the SeqClu class.\n",
    "        Args:\n",
    "            dataset (Dataset): The dataset that will be used for clustering.\n",
    "            nclasses (int): The number of classes/clusters.\n",
    "            nprototypes (int): The number of prototypes.\n",
    "        \"\"\"\n",
    "        assert dataset is not None\n",
    "        self.dataset = dataset\n",
    "        self.nclasses = nclasses\n",
    "        self.nprototypes = nprototypes\n",
    "        # Initialize the clusters\n",
    "        self.init_clusters = [self.dataset.points[i:i + nprototypes] for i in\n",
    "                              range(0, (nprototypes * nclasses) - 1, nprototypes)]\n",
    "        self.assigned_clusters = []\n",
    "        for i in range(0, nclasses):\n",
    "            for j in range(0, nprototypes):\n",
    "                self.assigned_clusters.append(i)\n",
    "        assert len(self.assigned_clusters) == nclasses * nprototypes\n",
    "        self.clustered = False\n",
    "        self.total_time = None\n",
    "        self.assignment_DTW = None\n",
    "        self.extra_DTW = None\n",
    "        self.silhouette_coefficient = None\n",
    "        self.f_measure = None\n",
    "        self.average_DTW = None\n",
    "\n",
    "    @abstractmethod\n",
    "    def clustering(self) -> None:\n",
    "        \"\"\"Abstract method which is used for clustering.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def calculateSilhouetteScore(self) -> None:\n",
    "        \"\"\"Method to calculate the silhouette score.\n",
    "        \"\"\"        \n",
    "        self.silhouette_coefficient = silhouette_score(self.dataset.points, self.assigned_clusters, metric=\"dtw\",\n",
    "                                                       n_jobs=-1)\n",
    "    \n",
    "    def calculateFmeasure(self) -> None:\n",
    "        \"\"\"Method to calculate the f-score.\n",
    "        \"\"\"        \n",
    "        self.f_measure = f1_score(self.dataset.labels, self.assigned_clusters, average='micro')\n",
    "\n",
    "    def calculateAverageDTW(self) -> None:\n",
    "        \"\"\"Calculates the average assignment DTW operations per point clustered.\n",
    "        \"\"\"        \n",
    "        if isinstance(self.assignment_DTW, int):\n",
    "            self.average_DTW = self.nclasses * self.nprototypes\n",
    "        else:\n",
    "            self.average_DTW = mean(self.assignment_DTW)\n",
    "\n",
    "    def getTotalTime(self) -> float:\n",
    "        \"\"\"Gets the total time to cluster all the points (excluding initialization).\n",
    "        Raises:\n",
    "            Exception: An exception is raised if the clustered has not been completed.\n",
    "        Returns:\n",
    "            float: The total time.\n",
    "        \"\"\"        \n",
    "        if not self.clustered:\n",
    "            raise Exception(\"Clustering hasn't been initiated\")\n",
    "        return self.total_time\n",
    "\n",
    "    def getSilhoutteScore(self) -> float:\n",
    "        \"\"\"Gets the silhouette score.\n",
    "        Raises:\n",
    "            Exception: An exception is raised if the clustered has not been completed.\n",
    "        Returns:\n",
    "            float: The silhouette score.\n",
    "        \"\"\"\n",
    "        if not self.clustered:\n",
    "            raise Exception(\"Clustering hasn't been initiated\")\n",
    "        if self.silhouette_coefficient is None:\n",
    "            self.calculateSilhouetteScore()\n",
    "        return self.silhouette_coefficient\n",
    "\n",
    "    def getFmeasure(self) -> float:\n",
    "        \"\"\"Gets the f-score.\n",
    "        Raises:\n",
    "            Exception: An exception is raised if the clustered has not been completed.\n",
    "        Returns:\n",
    "            float: The f-score.\n",
    "        \"\"\"\n",
    "        if not self.clustered:\n",
    "            raise Exception(\"Clustering hasn't been initiated\")\n",
    "        if self.f_measure is None:\n",
    "            self.calculateFmeasure()\n",
    "        return self.f_measure\n",
    "\n",
    "    def getAssignmentDTW(self) -> int:\n",
    "        \"\"\"Gets the total DTW operations during clustering.\n",
    "        Raises:\n",
    "            Exception: An exception is raised if the clustered has not been completed.\n",
    "        Returns:\n",
    "            float: The total DTW operating during clustering.\n",
    "        \"\"\"\n",
    "        if not self.clustered:\n",
    "            raise Exception(\"Clustering hasn't been initiated\")\n",
    "        elif isinstance(self.assignment_DTW, int):\n",
    "            return self.assignment_DTW\n",
    "        elif isinstance(self.assignment_DTW, list):\n",
    "            return sum(self.assignment_DTW)\n",
    "        raise Exception(\"assignmet_DTW initialized with wrong type. Actual type: \" + str(self.assignment_DTW))\n",
    "\n",
    "    def getExtraDTW(self) -> int:\n",
    "        \"\"\"Gets the extra DTW operations needed (during initialization). Always 0 for base and variant 1.\n",
    "        Raises:\n",
    "            Exception: An exception is raised if the clustered has not been completed.\n",
    "        Returns:\n",
    "            float: The extra DTW operations during initialization.\n",
    "        \"\"\"\n",
    "        if not self.clustered:\n",
    "            raise Exception(\"Clustering hasn't been initiated\")\n",
    "        return self.extra_DTW\n",
    "    \n",
    "    def getAverageDTW(self) -> float:\n",
    "        \"\"\"Gets the average DTW operations per clustered point.\n",
    "        Raises:\n",
    "            Exception: An exception is raised if the clustered has not been completed.\n",
    "        Returns:\n",
    "            float: The average DTW operations per clustered point.\n",
    "        \"\"\"\n",
    "        if not self.clustered:\n",
    "            raise Exception(\"Clustering hasn't been initiated\")\n",
    "        if self.average_DTW is None:\n",
    "            self.calculateAverageDTW()\n",
    "        return self.average_DTW\n",
    "\n",
    "def calculateDTW(p1: Any, p2: Any) -> float:\n",
    "   return fastdtw(p1, p2, dist=euclidean)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cluster:\n",
    "    \"\"\"The Cluster class is used to encapsulate the prototypes of the cluster. Used for Base and Variant 1.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, nprototypes: int, initial_prototypes: List) -> None:\n",
    "        \"\"\"The constructor for the Cluster class.\n",
    "        Args:\n",
    "            nprototypes (int): The number of prototypes.\n",
    "            initial_prototypes (List): The initial prototypes.\n",
    "        \"\"\"        \n",
    "        assert len(initial_prototypes) == nprototypes\n",
    "        self.prototypes = initial_prototypes\n",
    "        self.nprototypes = nprototypes\n",
    "\n",
    "    def shufflePrototypes(self) -> None:\n",
    "        \"\"\"Shuffles the prototypes of the cluster.\n",
    "        \"\"\"    \n",
    "        random.shuffle(self.prototypes)\n",
    "\n",
    "    def clusterUpdate(self, point: Any) -> None:\n",
    "        \"\"\"Updates the cluster prototypes with the new point\n",
    "        Args:\n",
    "            point (Any): The clustered point.\n",
    "        \"\"\"\n",
    "        maximum_distance = -1\n",
    "        maximum_idx = -1\n",
    "        for i in range(self.nprototypes):\n",
    "            distance = calculateDTW(self.prototypes[i], point)\n",
    "            if distance > maximum_distance:\n",
    "                maximum_idx = i\n",
    "                maximum_distance = distance\n",
    "        self.prototypes[maximum_idx] = point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SeqClu base implementation <a class=\"anchor\" id=\"base\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the original implementation of SeqClu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeqCluBase(SeqClu):\n",
    "    \"\"\"The original SeqClu implementation class.\n",
    "    Args:\n",
    "        SeqClu ([type]): Inherits from the SeqClu abstract class.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset: Dataset, nclasses: int, nprototypes: int) -> None:\n",
    "        \"\"\"The Constructor for the SeqCluBase class.\n",
    "        Args:\n",
    "            dataset (Dataset): The dataset that will be used for clustering.\n",
    "            nclasses (int): The number of classes/clusters.\n",
    "            nprototypes (int): The number of prototypes.\n",
    "        \"\"\"       \n",
    "        SeqClu.__init__(self, dataset, nclasses, nprototypes)\n",
    "        self.clusters = []\n",
    "        for cluster_prototypes in self.init_clusters:\n",
    "            self.clusters.append(Cluster(self.nprototypes, cluster_prototypes))\n",
    "\n",
    "        # Remove this attribute to free up memory since it will no longer be used.\n",
    "        del self.init_clusters\n",
    "\n",
    "        # The number of DTW calculations is known beforehand\n",
    "        self.assignment_DTW = self.nclasses * self.nprototypes * len(self.dataset.points)\n",
    "        self.extra_DTW = 0\n",
    "        self.clustering()\n",
    "\n",
    "    def clustering(self) -> None:\n",
    "        \"\"\"Clusters the points according to the original algorithm.\n",
    "        Raises:\n",
    "            Exception: An exception is raised if the clustering has already been done.\n",
    "        \"\"\"\n",
    "        if self.clustered:\n",
    "            raise Exception(\"Points have already been clustered\")\n",
    "        start_time = time.time()\n",
    "        for pidx, point in enumerate(self.dataset.points[self.nprototypes * self.nclasses:]):\n",
    "\n",
    "            # Cluster assignment phase: assigning cluster to a point\n",
    "            minimum_distance = math.inf\n",
    "            minimum_idx = -1\n",
    "            for cidx, cluster in enumerate(self.clusters):\n",
    "                distance = 0\n",
    "                for prototype in cluster.prototypes:\n",
    "                    distance = calculateDTW(prototype, point)\n",
    "                distance = distance * 1.0 / cluster.nprototypes * 1.0\n",
    "                if distance < minimum_distance:\n",
    "                    minimum_distance = distance\n",
    "                    minimum_idx = cidx\n",
    "\n",
    "            # Ensure that every point has been clustered\n",
    "            assert minimum_idx != -1\n",
    "            self.clusters[minimum_idx].clusterUpdate(point)\n",
    "            self.assigned_clusters.append(minimum_idx)\n",
    "        self.total_time = time.time() - start_time\n",
    "        assert len(self.dataset.points) == len(self.assigned_clusters)\n",
    "        self.clustered = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SeqClu new cluster assignment algorithm <a class=\"anchor\" id=\"new\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section contains the new algorithm for cluster assignment. There are three variants of this algorithms. These variants differ on how they select the prototypes at each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeqCluImproved(SeqClu):\n",
    "    \"\"\"The new improved SeqClu implementation class.\n",
    "    Args:\n",
    "        SeqClu ([type]): Inherits from the SeqClu abstract class.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset: Dataset, nclasses: int, nprototypes: int, phi: float) -> None:\n",
    "        \"\"\"The Constructor for the SeqCluImproved class.\n",
    "        Args:\n",
    "            dataset (Dataset): The dataset that will be used for clustering.\n",
    "            nclasses (int): The number of classes/clusters.\n",
    "            nprototypes (int): The number of prototypes.\n",
    "            phi (float): The phi value.\n",
    "        \"\"\"\n",
    "        SeqClu.__init__(self, dataset, nclasses, nprototypes)\n",
    "        self.phi = phi\n",
    "        self.extra_DTW = 0\n",
    "        self.assignment_DTW = []\n",
    "\n",
    "    def clustering(self) -> None:\n",
    "        \"\"\"Clusters the points according to the new improved algorithm algorithm.\n",
    "        Raises:\n",
    "            Exception: An exception is raised if the clustering has already been done.\n",
    "        \"\"\"\n",
    "        if self.clustered:\n",
    "            raise Exception(\"Points have already been clustered\")\n",
    "        start_time = time.time()\n",
    "        for pidx, point in enumerate(self.dataset.points[self.nprototypes * self.nclasses:]):\n",
    "            # Cluster assignment phase: assigning cluster to a point\n",
    "            max_iteration = self.nprototypes\n",
    "            number_DTW = 0\n",
    "            minimum_idx = -1\n",
    "\n",
    "            # Shuffle the prototypes for each cluster\n",
    "            # No shuffling is done for variants 2 & 3\n",
    "            self.shuffleClusterPrototypes()\n",
    "            selected_prototype_indices = {}\n",
    "            selected_prototype_distances = {}\n",
    "            for i in range(max_iteration):\n",
    "                number_DTW += self.selectPrototype(point, i, selected_prototype_distances, selected_prototype_indices)\n",
    "                if i == max_iteration - 1:\n",
    "                    pairs = list(selected_prototype_distances.items())\n",
    "                    minimum_distance = pairs[0][1]\n",
    "                    minimum_idx = pairs[0][0]\n",
    "                    for j in range(1, len(pairs)):\n",
    "                        current_distance = pairs[j][1]\n",
    "                        if current_distance < minimum_distance:\n",
    "                            minimum_distance = current_distance\n",
    "                            minimum_idx = pairs[j][0]\n",
    "                else:\n",
    "                    # Get the cluster id with the shortest distance to the incoming point\n",
    "                    closest_cluster = self.getClosestCluster(selected_prototype_distances)\n",
    "                    # Filter the cluster that are too far from the closest cluster\n",
    "                    new_selected_prototype_distances = {}\n",
    "                    new_selected_prototype_indices = {}\n",
    "                    for k, v in selected_prototype_distances.items():\n",
    "                        if k == closest_cluster:\n",
    "                            new_selected_prototype_distances[k] = v\n",
    "                            new_selected_prototype_indices[k] = selected_prototype_indices[k]\n",
    "                        else:\n",
    "                            closest_prototype = selected_prototype_distances[closest_cluster]\n",
    "                            # In case the distance is zero, take the small float number as smallest distance\n",
    "                            closest_prototype = 1.0e-200 if closest_prototype == 0.0 else closest_prototype\n",
    "                            if (v - closest_prototype) / closest_prototype * 1.0 <= self.phi:\n",
    "                                new_selected_prototype_distances[k] = v\n",
    "                                new_selected_prototype_indices[k] = selected_prototype_indices[k]\n",
    "                    # If one cluster remains, then assign to that cluster.\n",
    "                    if len(new_selected_prototype_distances) == 1:\n",
    "                        minimum_idx = list(new_selected_prototype_distances.items())[0][0]\n",
    "                        break\n",
    "                    # Otherwise continue with filtered clusters\n",
    "                    else:\n",
    "                        selected_prototype_distances = new_selected_prototype_distances\n",
    "                        selected_prototype_indices = new_selected_prototype_indices\n",
    "\n",
    "            # Ensure that every point has been clustered\n",
    "            assert minimum_idx != -1\n",
    "            self.assignment_DTW.append(number_DTW)\n",
    "            # Cluster update phase\n",
    "            self.clusters[minimum_idx].clusterUpdate(point)\n",
    "            self.assigned_clusters.append(minimum_idx)\n",
    "        assert len(self.dataset.points) == len(self.assigned_clusters)\n",
    "        self.total_time = time.time() - start_time\n",
    "        self.clustered = True\n",
    "    \n",
    "    @abstractmethod\n",
    "    def selectPrototype(self, point: Any, iteration: int, selected_prototype_distances: Dict[int, float],\n",
    "                        selected_prototype_indices: Dict[int, List[int]]) -> int:\n",
    "        \"\"\"Selects the next prototype for each iteration.\n",
    "        Args:\n",
    "            point (Any): The current point.\n",
    "            iteration (int): The current iteration\n",
    "            selected_prototype_distances (Dict[int, float]): The distance to the closest prototype for each cluster.\n",
    "            selected_prototype_indices (Dict[int, List[int]]): The indices of the selected prototypes of each cluster.\n",
    "        Returns:\n",
    "            int: The number of DTW operations done.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def shuffleClusterPrototypes(self) -> None:\n",
    "        \"\"\"Shuffles the prototypes of the cluster. Triggers only for Variant 1.\n",
    "        \"\"\"        \n",
    "        for cluster in self.clusters:\n",
    "            cluster.shufflePrototypes()\n",
    "\n",
    "    def getClosestCluster(self, selected_prototypes: Dict[int, float]) -> int:\n",
    "        \"\"\"Retrieves the cluster with the closest prototype.\n",
    "        Args:\n",
    "            selected_prototypes (Dict[int, float]): The distance to the closest prototype for each cluster.\n",
    "        Returns:\n",
    "            int: The closest cluster.\n",
    "        \"\"\"        \n",
    "        minimum = None\n",
    "        minimum_id = -1\n",
    "        pairs = [(k, v) for k, v in selected_prototypes.items()]\n",
    "        for i in range(len(pairs)):\n",
    "            if i == 0:\n",
    "                minimum = pairs[i][1]\n",
    "                minimum_id = pairs[i][0]\n",
    "            else:\n",
    "                c = pairs[i][1]\n",
    "                if c < minimum:\n",
    "                    minimum = c\n",
    "                    minimum_id = pairs[i][0]\n",
    "        return minimum_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SeqClu Variant 1 <a class=\"anchor\" id=\"variant1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variant 1 starts with selecting a random prototype from each cluster and continues selecting random prototypes in subsequent iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeqCluV1(SeqCluImproved):\n",
    "    \"\"\"The new improved SeqClu implementation class with variant 1 prototype selection.\n",
    "    Args:\n",
    "        SeqCluImproved ([type]): Inherits from the SeqCluImproved class.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset: Dataset, nclasses: int, nprototypes: int, phi: float) -> None:\n",
    "        \"\"\"The Constructor for the SeqCluV1 class.\n",
    "        Args:\n",
    "            dataset (Dataset): The dataset that will be used for clustering.\n",
    "            nclasses (int): The number of classes/clusters.\n",
    "            nprototypes (int): The number of prototypes.\n",
    "            phi (float): The phi value.\n",
    "        \"\"\"\n",
    "        SeqCluImproved.__init__(self, dataset, nclasses, nprototypes, phi)\n",
    "        self.clusters = []\n",
    "        for cluster_prototypes in self.init_clusters:\n",
    "            self.clusters.append(Cluster(self.nprototypes, cluster_prototypes))\n",
    "\n",
    "        # Remove this attribute to free up memory since it will no longer be used.\n",
    "        del self.init_clusters\n",
    "        self.clustering()\n",
    "    \n",
    "    def clustering(self) -> None:\n",
    "        \"\"\"Clusters the points according to the new improved algorithm algorithm.\n",
    "        Raises:\n",
    "            Exception: An exception is raised if the clustering has already been done.\n",
    "        \"\"\"\n",
    "        SeqCluImproved.clustering(self)\n",
    "    \n",
    "    def selectPrototype(self, point: Any, iteration: int, selected_prototype_distances: Dict[int, float],\n",
    "                        selected_prototype_indices: Dict[int, List[int]]) -> int:\n",
    "        \"\"\"Selects the next prototype for each iteration according to variant 1 selection.\n",
    "        Args:\n",
    "            point (Any): The current point.\n",
    "            iteration (int): The current iteration\n",
    "            selected_prototype_distances (Dict[int, float]): The distance to the closest prototype for each cluster.\n",
    "            selected_prototype_indices (Dict[int, List[int]]): The indices of the selected prototypes of each cluster.\n",
    "        Returns:\n",
    "            int: The number of DTW operations done.\n",
    "        \"\"\"\n",
    "        # All the prototypes were already randomized so they are selected iteratively.\n",
    "        number_DTW = 0\n",
    "        # For the first iteration, select the first prototype\n",
    "        if iteration == 0:\n",
    "            for cidx, cluster in enumerate(self.clusters):\n",
    "                p_d = calculateDTW(cluster.prototypes[iteration], point)\n",
    "                number_DTW += 1\n",
    "                selected_prototype_distances[cidx] = p_d\n",
    "                selected_prototype_indices[cidx] = [iteration]\n",
    "        # For subsequent iteration, only select a prototype if it is closer to the point than the currently selected one\n",
    "        else:\n",
    "            selected_clusters = selected_prototype_distances.keys()\n",
    "            for cidx, cluster in enumerate(self.clusters):\n",
    "                if cidx in selected_clusters:\n",
    "                    p_d = calculateDTW(cluster.prototypes[iteration], point)\n",
    "                    number_DTW += 1\n",
    "                    if p_d < selected_prototype_distances[cidx]:\n",
    "                        selected_prototype_distances[cidx] = p_d\n",
    "                        selected_prototype_indices[cidx].append(iteration)\n",
    "        return number_DTW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SeqClu Variant 2 <a class=\"anchor\" id=\"variant2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variant 2 starts with selecting a random prototype from each cluster but selects the nearest prototype to the last selected one which hasn't yet been selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClusterV2(Cluster):\n",
    "    \"\"\"The ClusterV2 class is used to encapsulate the prototypes of the cluster. Used for Variant 2.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, nprototypes: int, initial_prototypes: List) -> None:\n",
    "        \"\"\"The constructor for the ClusterV2 class.\n",
    "        Args:\n",
    "            nprototypes (int): The number of prototypes.\n",
    "            initial_prototypes (List): The initial prototypes.\n",
    "        \"\"\"\n",
    "        Cluster.__init__(self, nprototypes, initial_prototypes)\n",
    "        # Calculates the distances between the prototypes.\n",
    "        self.prototype_distances = []\n",
    "        for i in range(self.nprototypes):\n",
    "            distances = []\n",
    "            for j in range(self.nprototypes):\n",
    "                if i == j:\n",
    "                    distances.append(0.0)\n",
    "                elif i < j:\n",
    "                    distances.append(calculateDTW(self.prototypes[i], self.prototypes[j]))\n",
    "                elif i > j:\n",
    "                    distances.append(self.prototype_distances[j][i])\n",
    "            self.prototype_distances.append(distances)\n",
    "    \n",
    "    def shufflePrototypes(self) -> None:\n",
    "        \"\"\"No shuffling is needed for ClusterV2.\n",
    "        \"\"\"        \n",
    "        pass\n",
    "\n",
    "    def clusterUpdate(self, point: Any) -> None:\n",
    "        \"\"\"Updates the cluster prototypes with the new point\n",
    "        Args:\n",
    "            point (Any): The clustered point.\n",
    "        \"\"\"\n",
    "    \n",
    "        maximum_distance = -1\n",
    "        maximum_idx = -1\n",
    "        old_distances = []\n",
    "        for i in range(self.nprototypes):\n",
    "            distance = calculateDTW(self.prototypes[i], point)\n",
    "            old_distances.append(distance)\n",
    "            if distance > maximum_distance:\n",
    "                maximum_idx = i\n",
    "                maximum_distance = distance\n",
    "        self.prototypes[maximum_idx] = point\n",
    "        # Updates the distances matrix with the new point.\n",
    "        for i in range(self.nprototypes):\n",
    "            if i == maximum_idx:\n",
    "                continue\n",
    "            else:\n",
    "                distance = old_distances[i]\n",
    "                self.prototype_distances[maximum_idx][i] = distance\n",
    "                self.prototype_distances[i][maximum_idx] = distance\n",
    "    \n",
    "    def getPrototypeNeighbor(self, selected_prototypes: List[int]) -> int:\n",
    "        \"\"\"Method used to get the nearest prototype and that hasn't been selected.\n",
    "        Args:\n",
    "            selected_prototypes (List[int]): The prototypes selected.\n",
    "        Returns:\n",
    "            int: The id of the nearest not selected prototype.\n",
    "        \"\"\"        \n",
    "        maximum_distance = math.inf\n",
    "        maximum_idx = -1\n",
    "        prototype_id = selected_prototypes[len(selected_prototypes) - 1]\n",
    "        for i in range(self.nprototypes):\n",
    "            if i is not selected_prototypes:\n",
    "                distance = self.prototype_distances[prototype_id][i]\n",
    "                if distance < maximum_distance:\n",
    "                    maximum_distance = distance\n",
    "                    maximum_idx = i\n",
    "        return maximum_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeqCluV2(SeqCluImproved):\n",
    "    \"\"\"The new improved SeqClu implementation class with variant 2 prototype selection.\n",
    "    Args:\n",
    "        SeqCluImproved ([type]): Inherits from the SeqCluImproved class.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset: Dataset, nclasses: int, nprototypes: int, phi: float) -> None:\n",
    "        \"\"\"The Constructor for the SeqCluV2 class.\n",
    "        Args:\n",
    "            dataset (Dataset): The dataset that will be used for clustering.\n",
    "            nclasses (int): The number of classes/clusters.\n",
    "            nprototypes (int): The number of prototypes.\n",
    "            phi (float): The phi value.\n",
    "        \"\"\"\n",
    "        SeqCluImproved.__init__(self, dataset, nclasses, nprototypes, phi)\n",
    "        self.clusters = []\n",
    "        for cluster_prototypes in self.init_clusters:\n",
    "            self.clusters.append(ClusterV2(self.nprototypes, cluster_prototypes))\n",
    "\n",
    "        # Remove this attribute to free up memory since it will no longer be used.\n",
    "        del self.init_clusters\n",
    "\n",
    "        # Add the extra DTW calculations during initialization\n",
    "        self.extra_DTW = self.nclasses * math.comb(self.nprototypes, 2)\n",
    "        self.clustering()\n",
    "\n",
    "    def clustering(self) -> None:\n",
    "        \"\"\"Clusters the points according to the new improved algorithm algorithm.\n",
    "        Raises:\n",
    "            Exception: An exception is raised if the clustering has already been done.\n",
    "        \"\"\"\n",
    "        SeqCluImproved.clustering(self)\n",
    "\n",
    "    def selectPrototype(self, point: Any, iteration: int, selected_prototype_distances: Dict[int, float],\n",
    "                        selected_prototype_indices: Dict[int, List[int]]) -> int:\n",
    "        \"\"\"Selects the next prototype for each iteration according to variant 2 selection.\n",
    "        Args:\n",
    "            point (Any): The current point.\n",
    "            iteration (int): The current iteration\n",
    "            selected_prototype_distances (Dict[int, float]): The distance to the closest prototype for each cluster.\n",
    "            selected_prototype_indices (Dict[int, List[int]]): The indices of the selected prototypes of each cluster.\n",
    "        Returns:\n",
    "            int: The number of DTW operations done.\n",
    "        \"\"\"        \n",
    "        number_DTW = 0\n",
    "        # For the first iteration, select the first prototype\n",
    "        if iteration == 0:\n",
    "            for cidx, cluster in enumerate(self.clusters):\n",
    "                random_prototype = random.randint(0, self.nprototypes - 1)\n",
    "                p_d = calculateDTW(cluster.prototypes[random_prototype], point)\n",
    "                number_DTW += 1\n",
    "                selected_prototype_distances[cidx] = p_d\n",
    "                selected_prototype_indices[cidx] = [random_prototype]\n",
    "        # For subsequent iteration, only select a prototype if it is closer to the point than the currently selected one\n",
    "        else:\n",
    "            selected_clusters = selected_prototype_distances.keys()\n",
    "            for cidx, cluster in enumerate(self.clusters):\n",
    "                if cidx in selected_clusters:\n",
    "                    # Gets the nearest (non selected) prototype to the last selected one.\n",
    "                    next_prototype_id = cluster.getPrototypeNeighbor(selected_prototype_indices[cidx])\n",
    "                    p_d = calculateDTW(cluster.prototypes[next_prototype_id], point)\n",
    "                    number_DTW += 1\n",
    "                    if p_d < selected_prototype_distances[cidx]:\n",
    "                        selected_prototype_distances[cidx] = p_d\n",
    "                        selected_prototype_indices[cidx].append(next_prototype_id)\n",
    "        return number_DTW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SeqClu Variant 3 <a class=\"anchor\" id=\"variant3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variant 3 selects prototypes based on how central they are in each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClusterV3(Cluster):\n",
    "    \"\"\"The ClusterV3 class is used to encapsulate the prototypes of the cluster. Used for Variant 3.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, nprototypes: int, initial_prototypes: List) -> None:\n",
    "        \"\"\"The constructor for the ClusterV3 class.\n",
    "        Args:\n",
    "            nprototypes (int): The number of prototypes.\n",
    "            initial_prototypes (List): The initial prototypes.\n",
    "        \"\"\"\n",
    "        Cluster.__init__(self, nprototypes, initial_prototypes)\n",
    "        # Calculates the distances between the prototypes.\n",
    "        self.prototype_distances = []\n",
    "        for i in range(self.nprototypes):\n",
    "            distances = []\n",
    "            for j in range(self.nprototypes):\n",
    "                if i == j:\n",
    "                    distances.append(0.0)\n",
    "                elif i < j:\n",
    "                    distances.append(calculateDTW(self.prototypes[i], self.prototypes[j]))\n",
    "                elif i > j:\n",
    "                    distances.append(self.prototype_distances[j][i])\n",
    "            self.prototype_distances.append(distances)\n",
    "        # Sort the prototype.\n",
    "        self.sortPrototypes()\n",
    "\n",
    "    def shufflePrototypes(self) -> None:\n",
    "        \"\"\"No shuffling is needed for ClusterV2.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def sortPrototypes(self) -> None:\n",
    "        \"\"\"Sorts the prototypes based on how central they are.\n",
    "        \"\"\"        \n",
    "        prototype_average_distance = []\n",
    "        for i in range(len(self.prototype_distances)):\n",
    "            prototype_average_distance.append((i, sum(self.prototype_distances[i]) / (self.nprototypes - 1)))\n",
    "        prototype_average_distance.sort(key=operator.itemgetter(1))\n",
    "        new_order = [i[0] for i in prototype_average_distance]\n",
    "        self.prototypes = [self.prototypes[i] for i in new_order]\n",
    "\n",
    "    def clusterUpdate(self, point: Any) -> None:\n",
    "        \"\"\"Updates the cluster prototypes with the new point\n",
    "        Args:\n",
    "            point (Any): The clustered point.\n",
    "        \"\"\"\n",
    "        maximum_distance = -1\n",
    "        maximum_idx = -1\n",
    "        old_distances = []\n",
    "        for i in range(self.nprototypes):\n",
    "            distance = calculateDTW(self.prototypes[i], point)\n",
    "            old_distances.append(distance)\n",
    "            if distance > maximum_distance:\n",
    "                maximum_idx = i\n",
    "                maximum_distance = distance\n",
    "        self.prototypes[maximum_idx] = point\n",
    "        # Updates the distances matrix with the new point.\n",
    "        for i in range(self.nprototypes):\n",
    "            if i == maximum_idx:\n",
    "                continue\n",
    "            else:\n",
    "                distance = old_distances[i]\n",
    "                self.prototype_distances[maximum_idx][i] = distance\n",
    "                self.prototype_distances[i][maximum_idx] = distance\n",
    "        # Sort the prototypes again.\n",
    "        self.sortPrototypes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeqCluV3(SeqCluImproved):\n",
    "    \"\"\"The new improved SeqClu implementation class with variant 3 prototype selection.\n",
    "    Args:\n",
    "        SeqCluImproved ([type]): Inherits from the SeqCluImproved class.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset: Dataset, nclasses: int, nprototypes: int, phi: float) -> None:\n",
    "        \"\"\"The Constructor for the SeqCluV3 class.\n",
    "        Args:\n",
    "            dataset (Dataset): The dataset that will be used for clustering.\n",
    "            nclasses (int): The number of classes/clusters.\n",
    "            nprototypes (int): The number of prototypes.\n",
    "            phi (float): The phi value.\n",
    "        \"\"\"\n",
    "        SeqCluImproved.__init__(self, dataset, nclasses, nprototypes, phi)\n",
    "        self.clusters = []\n",
    "        for cluster_prototypes in self.init_clusters:\n",
    "            self.clusters.append(ClusterV3(self.nprototypes, cluster_prototypes))\n",
    "\n",
    "        # Remove this attribute to free up memory since it will no longer be used.\n",
    "        del self.init_clusters\n",
    "        self.extra_DTW = self.nclasses * math.comb(self.nprototypes, 2)\n",
    "        self.clustering()\n",
    "\n",
    "    def clustering(self) -> None:\n",
    "        \"\"\"Clusters the points according to the new improved algorithm algorithm.\n",
    "        Raises:\n",
    "            Exception: An exception is raised if the clustering has already been done.\n",
    "        \"\"\"\n",
    "        SeqCluImproved.clustering(self)\n",
    "\n",
    "    def selectPrototype(self, point: Any, iteration: int, selected_prototype_distances: Dict[int, float],\n",
    "                        selected_prototype_indices: Dict[int, List[int]]) -> int:\n",
    "        \"\"\"Selects the next prototype for each iteration according to variant 3 selection.\n",
    "        Args:\n",
    "            point (Any): The current point.\n",
    "            iteration (int): The current iteration\n",
    "            selected_prototype_distances (Dict[int, float]): The distance to the closest prototype for each cluster.\n",
    "            selected_prototype_indices (Dict[int, List[int]]): The indices of the selected prototypes of each cluster.\n",
    "        Returns:\n",
    "            int: The number of DTW operations done.\n",
    "        \"\"\"\n",
    "        number_DTW = 0\n",
    "        # Prototypes were already sorted to they are selected iteratively.\n",
    "        # For the first iteration, select the first prototype\n",
    "        if iteration == 0:\n",
    "            for cidx, cluster in enumerate(self.clusters):\n",
    "                p_d = calculateDTW(cluster.prototypes[iteration], point)\n",
    "                number_DTW += 1\n",
    "                selected_prototype_distances[cidx] = p_d\n",
    "                selected_prototype_indices[cidx] = [iteration]\n",
    "        # For subsequent iteration, only select a prototype if it is closer to the point than the currently selected one\n",
    "        else:\n",
    "            selected_clusters = selected_prototype_distances.keys()\n",
    "            for cidx, cluster in enumerate(self.clusters):\n",
    "                if cidx in selected_clusters:\n",
    "                    p_d = calculateDTW(cluster.prototypes[iteration], point)\n",
    "                    number_DTW += 1\n",
    "                    if p_d < selected_prototype_distances[cidx]:\n",
    "                        selected_prototype_distances[cidx] = p_d\n",
    "                        selected_prototype_indices[cidx].append(iteration)\n",
    "        return number_DTW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The experiment <a class=\"anchor\" id=\"experiment\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Experiment class receives the variants that will be used for the experiment and runs these variants multiple times while shuffling data at the end of each trial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Experiment:\n",
    "    \"\"\"The Experiment class used to run multiple iterations of different variants.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset: Dataset, nclasses: int, nprototypes: int, variants: List[Variant]) -> None:\n",
    "        \"\"\"The constructor for the Experiment class.\n",
    "        Args:\n",
    "            dataset (Dataset): The dataset to be used for the experiment.\n",
    "            nclasses (int): The number of classes.\n",
    "            nprototypes (int): The number of prototypes.\n",
    "            variants (List[Variant]): The list of variants for the experiments.\n",
    "        \"\"\"\n",
    "        # Ensure that the dataset is loaded.\n",
    "        assert dataset is not None\n",
    "        self.dataset = dataset\n",
    "        self.nclasses = nclasses\n",
    "        self.nprototypes = nprototypes\n",
    "        self.variants = variants\n",
    "        self.results: Dict[Variant, List[SeqClu]] = {}\n",
    "        for c in self.variants:\n",
    "            self.results[c] = []\n",
    "        self.trials = None\n",
    "        self.completed = False\n",
    "\n",
    "    def run_experiment(self, trials: int) -> None:\n",
    "        \"\"\"Runs the experiements for multuple trials with the preselected variants.\n",
    "        Args:\n",
    "            trials (int): The number of trials.\n",
    "        \"\"\"        \n",
    "        assert trials >= 2\n",
    "        self.trials = trials\n",
    "        assert not self.completed\n",
    "        pbar = tqdm(total= trials * len(self.results.keys()), position=0, leave=True)\n",
    "        for t in range(trials):\n",
    "            ds = self.dataset\n",
    "            shuffled_dataset = copy.deepcopy(ds)\n",
    "            shuffled_dataset.shuffle(self.nclasses, self.nprototypes)\n",
    "            assert shuffled_dataset is not None\n",
    "            for k in self.results.keys():\n",
    "                if k.variant == \"Original\":\n",
    "                    self.results[k].append(SeqCluBase(shuffled_dataset, self.nclasses, self.nprototypes))\n",
    "                if k.variant == \"Variant 1\":\n",
    "                    self.results[k].append(SeqCluV1(shuffled_dataset, self.nclasses, self.nprototypes, k.phi))\n",
    "                elif k.variant == \"Variant 2\":\n",
    "                    self.results[k].append(SeqCluV2(shuffled_dataset, self.nclasses, self.nprototypes, k.phi))\n",
    "                elif k.variant == \"Variant 3\":\n",
    "                    self.results[k].append(SeqCluV3(shuffled_dataset, self.nclasses, self.nprototypes, k.phi))\n",
    "                pbar.update(1)\n",
    "        pbar.close()\n",
    "        self.completed = True\n",
    "\n",
    "    def get_stats(self, metrics: List[str] = [\"Total DTW\", \"F score\"], to_print: bool = False,\n",
    "                  save_csv: bool = False) -> Dict[Variant, Stats]:\n",
    "        \"\"\"Calculates the mean and stdev for the metrics.\n",
    "        Args:\n",
    "            metrics (List[str], optional): The list of metrics to generate stats for. Defaults to [\"Total DTW\", \"F score\"].\n",
    "            to_print (bool, optional): Option to print the stats. Defaults to False.\n",
    "            save_csv (bool, optional): Option to save stats to csv. Defaults to False.\n",
    "        Raises:\n",
    "            Exception: Exception is raised if the experiments wan't run before.\n",
    "        Returns:\n",
    "            Dict[Variant, Stats]: The variants with their stats.\n",
    "        \"\"\"        \n",
    "        if not self.completed:\n",
    "            raise Exception(\"Run the experiment before executing this methode\")\n",
    "        stats = {}\n",
    "        for k, v in self.results.items():\n",
    "            measurements = {}\n",
    "            for m in metrics:\n",
    "                measure = None\n",
    "                if m == \"Total time\":\n",
    "                    measure = [i.getTotalTime() for i in v]\n",
    "                if m == \"Total DTW\":\n",
    "                    measure = [i.getAssignmentDTW() + i.getExtraDTW() for i in v]\n",
    "                if m == \"Silhouette\":\n",
    "                    measure = [i.getSilhoutteScore() for i in v]\n",
    "                if m == \"F score\":\n",
    "                    measure = [i.getFmeasure() for i in v]\n",
    "                if m == \"Accuracy\":\n",
    "                    measure = [i.getAccuracy() for i in v]\n",
    "                if m == \"Average DTW\":\n",
    "                    measure = [i.getAverageDTW() for i in v]\n",
    "                if m == \"Assignment DTW\":\n",
    "                    measure = [i.getAssignmentDTW() for i in v]\n",
    "                if m == \"Extra DTW\":\n",
    "                    measure = [i.getExtraDTW() for i in v]\n",
    "                measurements[m] = Measurements(m, measure)\n",
    "            stats[k] = Stats(str(k), measurements)\n",
    "        if to_print:\n",
    "            for k, v in stats.items():\n",
    "                print(str(v) + \"\\n\")\n",
    "        if save_csv:\n",
    "            self.save_csv(stats, metrics)\n",
    "        return stats\n",
    "\n",
    "    def save_csv(self, stats: Dict[Variant, Stats], metrics: List[str]) -> None:\n",
    "        \"\"\"Saves the stats to a csv file.\n",
    "        Args:\n",
    "            stats (Dict[Variant, Stats]): The stats for each variant.\n",
    "            metrics (List[str]): The metrics.\n",
    "        \"\"\"        \n",
    "        output = []\n",
    "        labels = [\"Implementation\"]\n",
    "        for m in metrics:\n",
    "            labels.append(\"Mean \" + m)\n",
    "            labels.append(\"Stdev \" + m)\n",
    "        for k, v in stats.items():\n",
    "            o = []\n",
    "            o.append(v.name)\n",
    "            for m in metrics:\n",
    "                o.append(v.measurements[m].mean)\n",
    "                o.append(v.measurements[m].std)\n",
    "            output.append(o)\n",
    "        df = pd.DataFrame(output, columns=labels)\n",
    "        df.to_csv(\"output\\\\\" + self.dataset.name + \"_\" + str(self.nclasses) + \"C_\" + str(self.nprototypes) + \"P.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothesis Testing <a class=\"anchor\" id=\"hypothesis_testing\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wilcoxon statistical test is carried out to check if there is a significant difference between two variants for different metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_metric(variant1: Variant, variant2: Variant, metric: str, stats: Dict[Variant, Stats]) -> None:\n",
    "    \"\"\"Function used to carry the Wilcoxon test between two variants.\n",
    "    Args:\n",
    "        variant1 (Variant): The first variant.\n",
    "        variant2 (Variant): The second variant.\n",
    "        metric (str): The metric.\n",
    "        stats (Dict[Variant, Stats]): The stats for each variant.\n",
    "    \"\"\"    \n",
    "    if variant1 in stats and variant2 in stats:\n",
    "        variant1_samples = stats[variant1].measurements[metric].samples\n",
    "        variant2_samples = stats[variant2].measurements[metric].samples\n",
    "        stat, p = wilcoxon(variant1_samples, variant2_samples)\n",
    "        print(\"---------------------------------------------------------\")\n",
    "        print('stat=%.3f, p=%.3f' % (stat, p))\n",
    "        if p > 0.05:\n",
    "            print(\"H0: There is no significant difference in \" + metric + \" between \" + str(variant1) + \" and \" + str(\n",
    "                variant2))\n",
    "        else:\n",
    "            print(\"H1: There is significant difference in \" + metric + \" between \" + str(variant1) + \" and \" + str(\n",
    "                variant2))\n",
    "        print(\"---------------------------------------------------------\")\n",
    "    else:\n",
    "        print(\"Trial not found. Run the experiment again\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hypothesis_testing(exp: Experiment, combs: List[Tuple[Variant, Variant]],\n",
    "                       metrics: List[str] = [\"Total time\", \"Total DTW\", \"Average DTW\", \"F score\"]) -> None:\n",
    "    \"\"\"Function used to carry Wilcoxon statistical test between pairs of variants for each metric.\n",
    "    Args:\n",
    "        exp (Experiment): The experiment object.\n",
    "        combs (List[Tuple[Variant, Variant]]): The list of pairs of variants to test.\n",
    "        metrics (List[str], optional): The list of metrics to test for. Defaults to [\"Total time\", \"Total DTW\", \"Average DTW\", \"F score\"].\n",
    "    \"\"\"    \n",
    "    stats = exp.get_stats(metrics=metrics)\n",
    "    for v1, v2 in combs:\n",
    "        for m in metrics:\n",
    "            test_metric(v1, v2, m, stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting results <a class=\"anchor\" id=\"plotting\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of the experiment are finally plotted as a bar chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_graph(exp: Experiment, combs: List[List[Variant]],\n",
    "               metrics: List[str] = [\"Total time\", \"Total DTW\", \"Average DTW\", \"F score\"]) -> None:\n",
    "    \"\"\"Plots the stats for particular list of variants\n",
    "    Args:\n",
    "        exp (Experiment): The experiment object.\n",
    "        combs (List[List[Variant]]): The variants to be plotted. Each list is plotted separately.\n",
    "        metrics (List[str], optional): The list of metrics. Defaults to [\"Total time\", \"Total DTW\", \"Average DTW\", \"F score\"].\n",
    "    \"\"\"    \n",
    "    stats = exp.get_stats(metrics=metrics + [\"Assignment DTW\", \"Extra DTW\"])\n",
    "    i = 0\n",
    "    plt.rcParams.update({\"font.size\": 500, \"font.weight\": \"bold\", \"figure.figsize\": (8, 6)})\n",
    "    plt.style.use(\"ggplot\")\n",
    "    plt.figure(dpi=1200)\n",
    "    for c in combs:\n",
    "        for m in metrics:\n",
    "            fig, ax = plt.subplots()\n",
    "            # Total DTW is plotted differently (uses Assignment DTW + Extra DTW)\n",
    "            if m == \"Total DTW\":\n",
    "                labels = []\n",
    "                assignment_means = []\n",
    "                extra_means = []\n",
    "                assignment_std = []\n",
    "                extra_std = []\n",
    "                width = 0.35\n",
    "                for sc in c:\n",
    "                    labels.append(str(sc))\n",
    "                    assignment_means.append(round(stats[sc].measurements[\"Assignment DTW\"].mean, 2))\n",
    "                    extra_means.append(round(stats[sc].measurements[\"Extra DTW\"].mean, 2))\n",
    "                    assignment_std.append(round(stats[sc].measurements[\"Assignment DTW\"].std, 2))\n",
    "                    extra_std.append(round(stats[sc].measurements[\"Extra DTW\"].std, 2))\n",
    "\n",
    "                p1 = ax.bar(labels, assignment_means, width, yerr=assignment_std, label=\"Assignment DTW\",\n",
    "                            align='center', ecolor='black', capsize=3)\n",
    "                p2 = ax.bar(labels, extra_means, width, bottom=assignment_means, label=\"Extra DTW\", align='center',\n",
    "                            ecolor='black', capsize=3)\n",
    "                ax.set_ylabel(\"Mean \" + m + \" calculations\")\n",
    "                ax.set_title(\"Total DTW calculations for \" + exp.dataset.name + \"\\n #samples=\" + str(\n",
    "                    exp.trials) + \", #clusters=\" + str(exp.nclasses) + \", #prototypes=\" + str(exp.nprototypes), pad=50)\n",
    "                ax.legend()\n",
    "                ax.bar_label(p1, label_type='center', padding=5, rotation=90)\n",
    "                # ax.bar_label(p2, label_type='center')\n",
    "                ax.bar_label(p2, padding=5, rotation=90)\n",
    "            else:\n",
    "                labels = []\n",
    "                means = []\n",
    "                std = []\n",
    "                width = 0.35\n",
    "                for sc in c:\n",
    "                    labels.append(str(sc))\n",
    "                    means.append(round(stats[sc].measurements[m].mean, 2))\n",
    "                    std.append(round(stats[sc].measurements[m].std, 2))\n",
    "                \n",
    "                p1 = ax.bar(labels, means, width, yerr=std, clip_on=False, align='center', ecolor='black', capsize=3)\n",
    "                ax.bar_label(p1, label_type='center', padding=5, rotation=90)\n",
    "                ax.set_ylabel(\"Mean \" + m)\n",
    "                ax.set_title(m + \" for \" + exp.dataset.name + \"\\n #samples=\" + str(exp.trials) + \", #clusters=\" + str(\n",
    "                    exp.nclasses) + \", #prototypes=\" + str(exp.nprototypes), pad=50)\n",
    "\n",
    "            ax.yaxis.grid(True)\n",
    "            plt.xticks(rotation=90)\n",
    "            # plt.autoscale()\n",
    "            plt.show()\n",
    "            fig.savefig(\"output\\\\\" + str(i) + \".png\", dpi=fig.dpi, bbox_inches=\"tight\")\n",
    "            i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution <a class=\"anchor\" id=\"execution\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the default list of variants that will be used for the experiments. It consists of the original SeqClu and the three variants of the new algorithm each using three values for phi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "org = Variant(\"Original\")\n",
    "variants = [org, Variant(\"Variant 1\", 0.75), Variant(\"Variant 1\", 1.0), Variant(\"Variant 1\", 1.25),\n",
    "            Variant(\"Variant 2\", 0.75), Variant(\"Variant 2\", 1.0), Variant(\"Variant 2\", 1.25),\n",
    "            Variant(\"Variant 3\", 0.75), Variant(\"Variant 3\", 1.0), Variant(\"Variant 3\", 1.25)]\n",
    "\n",
    "# Alternative combination used for testing\n",
    "# variants = [org, Variant(\"Variant 1\", 1.0), Variant(\"Variant 2\", 1.0), Variant(\"Variant 3\", 1.0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the parameters correctly. The fish dataset always has 7 classes. The number of classes for the handwritten dataset depends on the characters you choose.\n",
    "\n",
    "The default implementation of SeqClu uses 5 prototypes.\n",
    "\n",
    "Uncomment the relevant line for the dataset that you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nprototypes = 5 # Number of prototypes.\n",
    "# The toy dataset.\n",
    "# dataset = generateToyDataset(3, nprototypes)\n",
    "# The handwritten digits dataset\n",
    "dataset = generateHandWrittenDataset(nprototypes, classes = ['C', 'W', 'S', 'O', '1', '2', '6'])\n",
    "# The synthetic control dataset.\n",
    "# dataset = generateSynthicControlDS(nprototypes)\n",
    "nclasses = dataset.nclasses # Number of classes \n",
    "trials = 20 # Number of trials of the experiment\n",
    "exp = Experiment(dataset, nclasses, nprototypes, variants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.run_experiment(trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.get_stats(to_print=True, save_csv=True)\n",
    "plot_graph(exp, [variants])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypothesis_testing(exp, [(org, Variant(\"Variant 1\", 1.0)), (org, Variant(\"Variant 2\", 1.0)),\n",
    "                         (org, Variant(\"Variant 3\", 1.0)), (Variant(\"Variant 1\", 1.0), Variant(\"Variant 2\", 1.0)),\n",
    "                         (Variant(\"Variant 1\", 1.0), Variant(\"Variant 3\", 1.0)),\n",
    "                         (Variant(\"Variant 2\", 1.0), Variant(\"Variant 3\", 1.0))])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1baa965d5efe3ac65b79dfc60c0d706280b1da80fedb7760faf2759126c4f253"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}